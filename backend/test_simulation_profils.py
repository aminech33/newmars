"""
üß™ SIMULATION PROFILS √âL√àVES - MOTEUR LEAN v4.4 (10/10 ACAD√âMIQUE)
Test du moteur avec les 4 profils repr√©sentant ~95% des utilisateurs.

Profils:
1. D√âTERMIN√â (20%) - Motiv√© + capable ‚Üí atteint 100%
2. MOYEN (35%) - Motiv√© mais capacit√© normale ‚Üí le gros de la base
3. IRR√âGULIER (25%) - Capable mais motivation variable ‚Üí sessions espac√©es
4. EN DIFFICULT√â (20%) - Motivation ok mais capacit√© limit√©e ‚Üí a besoin d'aide

AM√âLIORATIONS v4.1:
- Oubli entre sessions (courbe d'Ebbinghaus)
- Transfert inter-topics (apprentissage connexe)
- Variabilit√© des questions (pi√®ges, questions faciles)
- Effet warmup (premi√®re question du jour)

AM√âLIORATIONS v4.2:
- Spacing effect (r√©vision espac√©e = meilleur apprentissage)
- Plateau naturel (stagnation √† 40-60% avant d√©clic)
- Confiance / Self-efficacy (diff√©rent de motivation)
- Streak motivation (jours cons√©cutifs = boost)
- Fatigue hebdomadaire (cumul sur la semaine)

AM√âLIORATIONS v4.3:
- Testing Effect (Roediger & Karpicke 2006): √™tre test√© > relire (+20% r√©tention)
- Interf√©rence proactive/r√©troactive: apprendre B perturbe temporairement A
- Effet Dunning-Kruger: surestimation des d√©butants, sous-estimation des experts
- Consolidation nocturne: le sommeil consolide la m√©moire (+10% le matin)

AM√âLIORATIONS v4.4 (10/10 ACAD√âMIQUE):
- Desirable Difficulties (Bjork 1994): difficult√© optimale = meilleur apprentissage
- Generation Effect (Slamecka 1978): g√©n√©rer > recevoir passivement
- Encoding Specificity (Tulving 1973): contexte apprentissage ‚âà contexte test
- Attention Fluctuante: vigilance varie toutes les ~20min (cycles ultradian)
"""

import sys
sys.path.insert(0, '.')

import random
import math
from datetime import datetime
from typing import Dict, List, Optional
from dataclasses import dataclass

from services.learning_engine_lean import lean_engine, LeanLearningEngine


# =============================================================================
# CONSTANTES R√âALISTES
# =============================================================================

# Matrice de transfert entre topics (apprendre A aide B)
# Valeur = % de transfert (0.2 = 20% du gain se transf√®re)
TOPIC_TRANSFER_MATRIX = {
    "conjugaison": {"grammaire": 0.15, "vocabulaire": 0.05, "orthographe": 0.10},
    "grammaire": {"conjugaison": 0.20, "vocabulaire": 0.05, "orthographe": 0.15},
    "vocabulaire": {"conjugaison": 0.05, "grammaire": 0.05, "orthographe": 0.25},
    "orthographe": {"conjugaison": 0.10, "grammaire": 0.10, "vocabulaire": 0.20},
}

# Taux d'oubli par jour (bas√© sur Ebbinghaus)
# retention = e^(-decay_rate * days_since_practice)
# 0.10 = ~10% d'oubli par jour sans pratique (r√©aliste pour apprentissage scolaire)
FORGETTING_DECAY_RATE = 0.10

# Variabilit√© des questions par difficult√©
# Certaines questions sont des "pi√®ges", d'autres plus intuitives
QUESTION_VARIANCE = {
    1: {"trap_prob": 0.05, "easy_prob": 0.30},  # Niveau 1: peu de pi√®ges
    2: {"trap_prob": 0.10, "easy_prob": 0.20},
    3: {"trap_prob": 0.15, "easy_prob": 0.15},  # Niveau 3: √©quilibr√©
    4: {"trap_prob": 0.20, "easy_prob": 0.10},
    5: {"trap_prob": 0.25, "easy_prob": 0.05},  # Niveau 5: beaucoup de pi√®ges
}

# Effet warmup (premi√®re question du jour)
WARMUP_PENALTY = 0.15  # -15% de chance de r√©ussir la premi√®re question

# =============================================================================
# NOUVELLES CONSTANTES v4.2 (10/10)
# =============================================================================

# Spacing Effect - R√©viser apr√®s 2-3 jours est MIEUX que tous les jours
# Bas√© sur la recherche de Cepeda et al. (2006)
# Ajust√© pour ne pas trop p√©naliser la pratique quotidienne
SPACING_EFFECT = {
    0: 0.95,   # M√™me jour = massed practice = -5% (l√©ger)
    1: 1.00,   # 1 jour = normal
    2: 1.10,   # 2 jours = optimal = +10%
    3: 1.10,   # 3 jours = optimal = +10%
    4: 1.05,   # 4 jours = bon = +5%
    5: 1.00,   # 5 jours = normal
}  # Au-del√† de 5 jours = 1.0 (normal)

# Plateau naturel - Probabilit√© de stagnation √† certains seuils
# Bas√© sur la "courbe en S" de l'apprentissage
PLATEAU_CONFIG = {
    "zone": (0.35, 0.65),      # Zone de plateau: 35-65% de ma√Ætrise
    "probability": 0.15,       # 15% de chance d'√™tre en plateau chaque jour
    "duration_min": 2,         # Minimum 2 jours de plateau
    "duration_max": 5,         # Maximum 5 jours de plateau
    "breakthrough_prob": 0.20, # 20% de chance de "d√©clic" par jour en plateau
}

# Confiance / Self-efficacy
# Diff√©rent de motivation: affecte la prise de risque et la persistence
CONFIDENCE_CONFIG = {
    "initial": 0.50,           # Confiance initiale
    "gain_on_success": 0.03,   # +3% par r√©ponse correcte
    "loss_on_failure": 0.05,   # -5% par r√©ponse incorrecte
    "streak_bonus": 0.02,      # +2% par r√©ponse correcte cons√©cutive
    "min": 0.10,               # Plancher
    "max": 0.95,               # Plafond
}

# Streak motivation - Jours cons√©cutifs d'√©tude
STREAK_MOTIVATION = {
    3: 0.05,   # 3 jours cons√©cutifs = +5% motivation
    5: 0.08,   # 5 jours = +8%
    7: 0.12,   # 7 jours = +12%
    14: 0.15,  # 14 jours = +15%
}
STREAK_BREAK_PENALTY = 0.10  # Perdre son streak = -10% motivation temporaire

# Fatigue hebdomadaire - Cumul sur la semaine
WEEKLY_FATIGUE_CONFIG = {
    "sessions_threshold": 5,   # √Ä partir de 5 sessions/semaine
    "fatigue_per_extra": 0.05, # +5% fatigue de base par session au-del√†
    "weekend_recovery": 0.50,  # R√©cup√©ration de 50% de la fatigue le weekend
    "max_weekly_fatigue": 0.30, # Maximum 30% de fatigue hebdomadaire
}

# =============================================================================
# NOUVELLES CONSTANTES v4.3 (VRAI 10/10)
# =============================================================================

# Testing Effect (Roediger & Karpicke 2006)
# √ätre test√© am√©liore la r√©tention de 20% vs simplement relire
# Appliqu√©: bonus d'apprentissage quand on r√©pond (m√™me incorrectement)
TESTING_EFFECT_CONFIG = {
    "retention_boost": 0.20,      # +20% de r√©tention gr√¢ce au testing
    "retrieval_practice_bonus": 0.15,  # Bonus si on "r√©cup√®re" activement l'info
    "passive_decay": 0.05,        # P√©nalit√© si on ne fait que lire sans tester
}

# Interf√©rence proactive et r√©troactive
# Proactive: ce qu'on savait AVANT interf√®re avec le nouveau
# R√©troactive: ce qu'on apprend APR√àS perturbe l'ancien
INTERFERENCE_CONFIG = {
    "proactive_rate": 0.08,       # Interf√©rence de l'ancien sur le nouveau
    "retroactive_rate": 0.05,    # Interf√©rence du nouveau sur l'ancien
    "similarity_threshold": 0.3,  # Seuil de similarit√© pour d√©clencher l'interf√©rence
    "decay_per_day": 0.02,       # L'interf√©rence diminue avec le temps
}

# Effet Dunning-Kruger
# D√©butants surestiment leurs comp√©tences, experts sous-estiment
# Affecte la confiance PER√áUE vs la confiance R√âELLE
DUNNING_KRUGER_CONFIG = {
    "novice_zone": (0.0, 0.25),   # Zone de surestimation
    "novice_overconfidence": 0.25, # +25% de confiance per√ßue
    "competent_zone": (0.40, 0.60), # Zone de sous-estimation ("vall√©e du d√©sespoir")
    "competent_underconfidence": -0.15, # -15% de confiance per√ßue
    "expert_zone": (0.80, 1.0),   # Zone de l√©g√®re sous-estimation
    "expert_underconfidence": -0.05, # -5% de confiance per√ßue
}

# Consolidation nocturne (Walker 2017, "Why We Sleep")
# Le sommeil consolide la m√©moire: +10% d'efficacit√© le matin apr√®s sommeil
SLEEP_CONSOLIDATION_CONFIG = {
    "morning_bonus": 0.10,        # +10% d'efficacit√© le matin (apr√®s sommeil)
    "evening_fatigue": 0.05,      # +5% de fatigue le soir
    "consolidation_boost": 0.08,  # +8% de r√©tention apr√®s une nuit de sommeil
    "morning_hours": (6, 12),     # Heures du "matin" (bonus)
    "evening_hours": (18, 23),    # Heures du "soir" (malus)
}

# =============================================================================
# NOUVELLES CONSTANTES v4.4 (10/10 ACAD√âMIQUE)
# =============================================================================

# Desirable Difficulties (Bjork 1994)
# Une difficult√© "d√©sirable" am√©liore l'apprentissage long terme
# Trop facile = pas d'effort = oubli rapide
# Trop dur = frustration = abandon
# Juste assez dur = effort optimal = r√©tention maximale
DESIRABLE_DIFFICULTY_CONFIG = {
    "optimal_error_rate": (0.15, 0.30),  # 15-30% d'erreurs = zone optimale
    "too_easy_penalty": 0.15,            # -15% d'apprentissage si trop facile
    "too_hard_penalty": 0.20,            # -20% d'apprentissage si trop dur
    "optimal_bonus": 0.20,               # +20% d'apprentissage en zone optimale
    "variability_bonus": 0.10,           # +10% si on varie les types de questions
}

# Generation Effect (Slamecka & Graf 1978)
# G√©n√©rer une r√©ponse (m√™me incorrecte) > recevoir passivement
# Le cerveau encode mieux ce qu'il a activement produit
GENERATION_EFFECT_CONFIG = {
    "active_generation_bonus": 0.15,     # +15% si g√©n√©ration active (vs QCM)
    "partial_generation_bonus": 0.08,    # +8% si r√©ponse partielle
    "passive_penalty": 0.05,             # -5% si r√©ponse donn√©e sans effort
    "elaboration_bonus": 0.10,           # +10% si explication demand√©e
}

# Encoding Specificity (Tulving & Thomson 1973)
# On se souvient mieux si le contexte de test ‚âà contexte d'apprentissage
# Simul√©: consistance du contexte (heure, lieu, √©tat)
ENCODING_SPECIFICITY_CONFIG = {
    "context_match_bonus": 0.12,         # +12% si m√™me contexte
    "context_mismatch_penalty": 0.08,    # -8% si contexte diff√©rent
    "time_consistency_weight": 0.4,      # Poids de l'heure dans le contexte
    "state_consistency_weight": 0.6,     # Poids de l'√©tat (fatigue/motivation)
}

# Attention Fluctuante (Cycles Ultradian)
# L'attention varie naturellement toutes les ~20 minutes
# Bas√© sur les rythmes ultradian (90-120 min avec pics/creux de 20min)
ATTENTION_FLUCTUATION_CONFIG = {
    "cycle_duration": 20,                # Minutes par cycle d'attention
    "peak_bonus": 0.10,                  # +10% au pic d'attention
    "trough_penalty": 0.15,              # -15% au creux d'attention
    "recovery_questions": 3,             # Questions pour r√©cup√©rer apr√®s un creux
    "micro_break_benefit": 0.05,         # +5% apr√®s une micro-pause
}


# =============================================================================
# D√âFINITION DES 4 PROFILS
# =============================================================================

@dataclass
class StudentProfile:
    """Profil d'un √©l√®ve"""
    name: str
    description: str
    # Capacit√©s cognitives
    base_ability: float  # 0-1, capacit√© de base √† r√©pondre correctement
    learning_speed: float  # Vitesse d'apprentissage (multiplicateur)
    # Motivation
    motivation: float  # 0-1, motivation g√©n√©rale
    motivation_variance: float  # Variance de motivation (0 = stable, 0.3 = variable)
    # Comportement
    focus: float  # 0-1, concentration
    fatigue_resistance: float  # 0-1, r√©sistance √† la fatigue
    # Sessions
    session_skip_prob: float  # Probabilit√© de sauter une session (0-1)
    early_quit_prob: float  # Probabilit√© d'abandonner en cours de session


# Les 4 profils cl√©s (AJUST√âS pour r√©alisme)
PROFILES = {
    "determined": StudentProfile(
        name="Marie (D√©termin√©e)",
        description="Motiv√©e, capable, r√©guli√®re. L'√©l√®ve id√©ale.",
        base_ability=0.70,  # L√©g√®rement r√©duit
        learning_speed=1.1,
        motivation=0.85,
        motivation_variance=0.05,
        focus=0.80,
        fatigue_resistance=0.75,
        session_skip_prob=0.03,
        early_quit_prob=0.02
    ),
    "average": StudentProfile(
        name="Salim (Moyen)",
        description="Motiv√© mais capacit√© normale. Repr√©sente 35% des utilisateurs.",
        base_ability=0.50,  # Capacit√© moyenne
        learning_speed=0.9,  # L√©g√®rement plus lent
        motivation=0.65,
        motivation_variance=0.15,
        focus=0.60,
        fatigue_resistance=0.55,
        session_skip_prob=0.08,
        early_quit_prob=0.04
    ),
    "irregular": StudentProfile(
        name="Emma (Irr√©guli√®re)",
        description="Capable mais motivation variable. Sessions espac√©es.",
        base_ability=0.65,
        learning_speed=1.0,
        motivation=0.50,
        motivation_variance=0.25,  # Moins de variance
        focus=0.65,
        fatigue_resistance=0.65,
        session_skip_prob=0.25,  # R√©duit de 35% √† 25%
        early_quit_prob=0.10  # R√©duit
    ),
    "struggling": StudentProfile(
        name="Tom (En difficult√©)",
        description="Motiv√© mais capacit√© limit√©e. A besoin d'adaptation.",
        base_ability=0.40,  # Capacit√© limit√©e
        learning_speed=0.70,
        motivation=0.70,  # Bien motiv√©!
        motivation_variance=0.10,  # Stable
        focus=0.55,
        fatigue_resistance=0.50,
        session_skip_prob=0.05,  # Tr√®s r√©gulier car motiv√©
        early_quit_prob=0.06  # Reste jusqu'au bout
    )
}


class SimulatedStudent:
    """
    √âl√®ve simul√© bas√© sur un profil.

    AM√âLIORATIONS v4.1:
    - Oubli entre sessions (Ebbinghaus)
    - Transfert inter-topics
    - Variabilit√© des questions
    - Effet warmup

    AM√âLIORATIONS v4.2:
    - Spacing effect
    - Plateau naturel
    - Confiance / Self-efficacy
    - Streak motivation
    - Fatigue hebdomadaire

    AM√âLIORATIONS v4.3 (VRAI 10/10):
    - Testing Effect
    - Interf√©rence proactive/r√©troactive
    - Effet Dunning-Kruger
    - Consolidation nocturne

    AM√âLIORATIONS v4.4 (10/10 ACAD√âMIQUE):
    - Desirable Difficulties (Bjork 1994)
    - Generation Effect (Slamecka 1978)
    - Encoding Specificity (Tulving 1973)
    - Attention Fluctuante (cycles ultradian)
    """

    def __init__(self, profile: StudentProfile):
        self.profile = profile
        self.knowledge: Dict[str, float] = {}
        self.current_motivation = profile.motivation
        self.fatigue = 0.0
        self.session_questions = 0
        # v4.1: tracking pour oubli
        self.last_practice_day: Dict[str, int] = {}  # topic -> dernier jour pratiqu√©
        self.current_day = 0
        # v4.1: tracking warmup
        self.is_first_question_of_day = True

        # v4.2: Confiance / Self-efficacy
        self.confidence = CONFIDENCE_CONFIG["initial"]
        self.consecutive_correct = 0  # Pour streak de confiance

        # v4.2: Streak motivation (jours cons√©cutifs)
        self.study_streak = 0
        self.last_study_day = -1
        self.streak_broken_today = False  # Pour p√©nalit√© temporaire

        # v4.2: Fatigue hebdomadaire
        self.weekly_sessions = 0
        self.weekly_fatigue = 0.0
        self.current_week = 0

        # v4.2: Plateau naturel
        self.in_plateau = False
        self.plateau_days_remaining = 0
        self.had_breakthrough = False  # D√©clic d√©j√† eu?

        # v4.3: Testing Effect - tracking des topics test√©s
        self.tested_topics: Dict[str, int] = {}  # topic -> nombre de tests
        self.retrieval_attempts: Dict[str, int] = {}  # tentatives de r√©cup√©ration

        # v4.3: Interf√©rence - tracking des topics r√©cemment appris
        self.recent_learning: List[tuple] = []  # [(topic, day, gain), ...]
        self.interference_levels: Dict[str, float] = {}  # topic -> niveau d'interf√©rence actuel

        # v4.3: Dunning-Kruger - confiance per√ßue vs r√©elle
        self.perceived_confidence = CONFIDENCE_CONFIG["initial"]

        # v4.3: Consolidation nocturne - tracking du sommeil
        self.last_session_hour = 10  # Heure par d√©faut (matin)
        self.sleep_consolidated = False  # A-t-on b√©n√©fici√© de la consolidation aujourd'hui?

        # v4.4: Desirable Difficulties - tracking du taux d'erreur r√©cent
        self.recent_errors: List[bool] = []  # True = erreur, False = correct (derni√®res 10 questions)
        self.question_types_used: List[str] = []  # Pour variabilit√© des types

        # v4.4: Generation Effect - type de r√©ponse
        self.generation_mode = "active"  # "active" (g√©n√®re), "passive" (QCM), "partial"

        # v4.4: Encoding Specificity - contexte d'apprentissage
        self.learning_contexts: Dict[str, dict] = {}  # topic -> {hour, motivation, fatigue}

        # v4.4: Attention Fluctuante - cycle ultradian
        self.session_start_minute = 0  # Minute de d√©but de session
        self.questions_since_trough = 0  # Questions depuis dernier creux
        self.in_attention_trough = False  # Dans un creux d'attention?

    def start_session(self, day: int, hour: int = None):
        """D√©but de session - reset fatigue, ajuster motivation, appliquer tous les effets"""
        self.session_questions = 0
        self.current_day = day
        self.is_first_question_of_day = True

        # v4.3: Simuler l'heure de la session (pour consolidation nocturne)
        if hour is None:
            # Simuler une heure r√©aliste (pic entre 9h et 20h)
            hour = random.choice([9, 10, 11, 14, 15, 16, 17, 18, 19, 20])
        self.last_session_hour = hour

        # Calculer la semaine (pour fatigue hebdomadaire)
        new_week = day // 7
        if new_week > self.current_week:
            # Nouvelle semaine - r√©cup√©ration weekend
            self._apply_weekend_recovery()
            self.current_week = new_week
            self.weekly_sessions = 0

        # Incr√©menter sessions hebdomadaires
        self.weekly_sessions += 1

        # v4.2: G√©rer le streak
        self._update_streak(day)

        # v4.2: Calculer fatigue de base (quotidienne + hebdomadaire)
        self.fatigue = self._calculate_base_fatigue()

        # v4.3: Ajuster fatigue selon l'heure (consolidation nocturne)
        self.fatigue += self._get_time_of_day_fatigue()

        # Variance de motivation + bonus streak
        variance = random.uniform(-self.profile.motivation_variance, self.profile.motivation_variance)
        base_motivation = self.profile.motivation + variance

        # v4.2: Bonus streak motivation
        streak_bonus = self._get_streak_motivation_bonus()

        # v4.2: P√©nalit√© si streak cass√© aujourd'hui
        streak_penalty = STREAK_BREAK_PENALTY if self.streak_broken_today else 0

        self.current_motivation = max(0.2, min(1.0, base_motivation + streak_bonus - streak_penalty))
        self.streak_broken_today = False  # Reset pour demain

        # v4.1: Appliquer l'oubli (courbe d'Ebbinghaus)
        self._apply_forgetting(day)

        # v4.3: Appliquer la consolidation nocturne (si nouveau jour)
        if day > 0:
            self._apply_sleep_consolidation()

        # v4.3: Diminuer l'interf√©rence avec le temps
        self._decay_interference()

        # v4.2: G√©rer le plateau
        self._update_plateau_state()

        # v4.4: Initialiser le cycle d'attention (ultradian)
        self.session_start_minute = random.randint(0, 19)  # Position dans le cycle de 20 min
        self.questions_since_trough = 0
        self.in_attention_trough = False

    def _apply_forgetting(self, current_day: int):
        """
        Applique l'oubli bas√© sur la courbe d'Ebbinghaus.
        retention = e^(-decay_rate * days_since_practice)

        Plus on a pratiqu√© r√©cemment, moins on oublie.
        Plus la ma√Ætrise est √©lev√©e, plus la r√©tention est bonne.
        """
        for topic in list(self.knowledge.keys()):
            last_day = self.last_practice_day.get(topic, 0)
            days_since = current_day - last_day

            if days_since <= 0:
                continue  # Pas d'oubli si pratiqu√© aujourd'hui

            current_mastery = self.knowledge[topic]

            # Facteur de r√©tention bas√© sur la ma√Ætrise
            # Plus on ma√Ætrise, moins on oublie vite
            mastery_factor = 0.5 + current_mastery * 0.5  # 0.5 √† 1.0

            # Calcul de la r√©tention (Ebbinghaus modifi√©)
            effective_decay = FORGETTING_DECAY_RATE / mastery_factor
            retention = math.exp(-effective_decay * days_since)

            # Plancher: on ne descend jamais en dessous de 30% de la ma√Ætrise initiale
            min_retention = 0.30
            retention = max(min_retention, retention)

            # Appliquer l'oubli
            new_mastery = current_mastery * retention
            self.knowledge[topic] = max(0.0, new_mastery)

    def apply_day_skip(self, day: int):
        """Appel√© quand l'√©l√®ve saute une session - applique l'oubli et casse le streak"""
        self._apply_forgetting(day)

        # v4.2: Casser le streak si on saute un jour
        if self.last_study_day >= 0 and day > self.last_study_day + 1:
            if self.study_streak >= 3:  # P√©nalit√© seulement si on avait un streak significatif
                self.streak_broken_today = True
            self.study_streak = 0

        self.current_day = day

    # =========================================================================
    # v4.2: NOUVELLES M√âTHODES
    # =========================================================================

    def _update_streak(self, day: int):
        """Met √† jour le streak de jours cons√©cutifs"""
        if self.last_study_day == day - 1:
            # Jour cons√©cutif
            self.study_streak += 1
        elif self.last_study_day < day - 1:
            # Streak cass√©
            if self.study_streak >= 3:
                self.streak_broken_today = True
            self.study_streak = 1  # Recommence √† 1 aujourd'hui

        self.last_study_day = day

    def _get_streak_motivation_bonus(self) -> float:
        """Calcule le bonus de motivation bas√© sur le streak"""
        bonus = 0.0
        for threshold, value in sorted(STREAK_MOTIVATION.items(), reverse=True):
            if self.study_streak >= threshold:
                bonus = value
                break
        return bonus

    def _calculate_base_fatigue(self) -> float:
        """Calcule la fatigue de base incluant la fatigue hebdomadaire"""
        # Fatigue hebdomadaire si trop de sessions
        config = WEEKLY_FATIGUE_CONFIG
        extra_sessions = max(0, self.weekly_sessions - config["sessions_threshold"])
        self.weekly_fatigue = min(
            config["max_weekly_fatigue"],
            extra_sessions * config["fatigue_per_extra"]
        )
        return self.weekly_fatigue  # La fatigue quotidienne s'ajoute pendant la session

    def _apply_weekend_recovery(self):
        """R√©cup√©ration de fatigue pendant le weekend"""
        recovery = WEEKLY_FATIGUE_CONFIG["weekend_recovery"]
        self.weekly_fatigue *= (1 - recovery)

    def _update_plateau_state(self):
        """G√®re l'√©tat de plateau (stagnation)"""
        avg_mastery = sum(self.knowledge.values()) / max(1, len(self.knowledge)) if self.knowledge else 0

        zone_min, zone_max = PLATEAU_CONFIG["zone"]

        # V√©rifier si on est dans la zone de plateau
        if zone_min <= avg_mastery <= zone_max and not self.had_breakthrough:
            if self.in_plateau:
                # D√©j√† en plateau - v√©rifier breakthrough
                self.plateau_days_remaining -= 1
                if random.random() < PLATEAU_CONFIG["breakthrough_prob"]:
                    # D√©clic!
                    self.in_plateau = False
                    self.had_breakthrough = True
                elif self.plateau_days_remaining <= 0:
                    # Fin naturelle du plateau
                    self.in_plateau = False
            else:
                # Pas encore en plateau - v√©rifier si on y entre
                if random.random() < PLATEAU_CONFIG["probability"]:
                    self.in_plateau = True
                    self.plateau_days_remaining = random.randint(
                        PLATEAU_CONFIG["duration_min"],
                        PLATEAU_CONFIG["duration_max"]
                    )
        else:
            # Hors zone de plateau
            self.in_plateau = False

    def _get_spacing_bonus(self, topic: str) -> float:
        """Calcule le bonus de spacing effect pour un topic"""
        last_day = self.last_practice_day.get(topic, -10)
        days_since = self.current_day - last_day

        if days_since in SPACING_EFFECT:
            return SPACING_EFFECT[days_since]
        return 1.0  # Pas de bonus/malus au-del√† de 5 jours

    def _update_confidence(self, is_correct: bool):
        """Met √† jour la confiance apr√®s une r√©ponse"""
        config = CONFIDENCE_CONFIG

        if is_correct:
            self.consecutive_correct += 1
            gain = config["gain_on_success"]
            # Bonus streak de confiance
            if self.consecutive_correct >= 3:
                gain += config["streak_bonus"]
            self.confidence = min(config["max"], self.confidence + gain)
        else:
            self.consecutive_correct = 0
            self.confidence = max(config["min"], self.confidence - config["loss_on_failure"])

        # v4.3: Mettre √† jour la confiance per√ßue (Dunning-Kruger)
        self._update_perceived_confidence()

    # =========================================================================
    # v4.3: NOUVELLES M√âTHODES (VRAI 10/10)
    # =========================================================================

    def _get_time_of_day_fatigue(self) -> float:
        """Calcule la fatigue additionnelle selon l'heure (consolidation nocturne)"""
        config = SLEEP_CONSOLIDATION_CONFIG
        hour = self.last_session_hour

        if config["morning_hours"][0] <= hour <= config["morning_hours"][1]:
            # Matin: bonus (moins de fatigue)
            return -config["morning_bonus"]
        elif config["evening_hours"][0] <= hour <= config["evening_hours"][1]:
            # Soir: malus (plus de fatigue)
            return config["evening_fatigue"]
        return 0.0

    def _apply_sleep_consolidation(self):
        """Applique le bonus de consolidation nocturne (le sommeil consolide la m√©moire)"""
        if self.sleep_consolidated:
            return  # D√©j√† appliqu√© aujourd'hui

        config = SLEEP_CONSOLIDATION_CONFIG
        boost = config["consolidation_boost"]

        # Appliquer un petit boost √† tous les topics (le sommeil consolide)
        for topic in self.knowledge:
            # Le boost est proportionnel √† ce qui a √©t√© appris r√©cemment
            if topic in self.last_practice_day:
                days_since = self.current_day - self.last_practice_day[topic]
                if days_since == 1:  # Pratiqu√© hier = consolidation maximale
                    self.knowledge[topic] = min(1.0, self.knowledge[topic] * (1 + boost))

        self.sleep_consolidated = True

    def _decay_interference(self):
        """Diminue l'interf√©rence avec le temps"""
        config = INTERFERENCE_CONFIG
        decay = config["decay_per_day"]

        for topic in list(self.interference_levels.keys()):
            self.interference_levels[topic] = max(0, self.interference_levels[topic] - decay)
            if self.interference_levels[topic] <= 0:
                del self.interference_levels[topic]

        # Nettoyer les apprentissages r√©cents (> 3 jours)
        self.recent_learning = [
            (t, d, g) for t, d, g in self.recent_learning
            if self.current_day - d <= 3
        ]

    def _apply_proactive_interference(self, topic: str) -> float:
        """
        Interf√©rence proactive: ce qu'on savait AVANT interf√®re avec le nouveau.
        Retourne un malus (n√©gatif) si interf√©rence d√©tect√©e.
        """
        config = INTERFERENCE_CONFIG

        # Chercher des topics similaires d√©j√† ma√Ætris√©s
        interference = 0.0
        for other_topic, mastery in self.knowledge.items():
            if other_topic == topic:
                continue
            # Plus on ma√Ætrise un topic similaire, plus il peut interf√©rer
            similarity = TOPIC_TRANSFER_MATRIX.get(topic, {}).get(other_topic, 0)
            if similarity >= config["similarity_threshold"]:
                interference += mastery * config["proactive_rate"] * similarity

        return -min(0.15, interference)  # Plafonn√© √† -15%

    def _apply_retroactive_interference(self, topic: str, gain: float):
        """
        Interf√©rence r√©troactive: apprendre quelque chose de nouveau perturbe l'ancien.
        Applique une petite perte aux topics similaires.
        """
        config = INTERFERENCE_CONFIG

        if gain <= 0:
            return

        # Enregistrer cet apprentissage
        self.recent_learning.append((topic, self.current_day, gain))

        # Appliquer l'interf√©rence aux topics similaires
        for other_topic in self.knowledge:
            if other_topic == topic:
                continue
            similarity = TOPIC_TRANSFER_MATRIX.get(other_topic, {}).get(topic, 0)
            if similarity >= config["similarity_threshold"]:
                interference = gain * config["retroactive_rate"] * similarity
                self.interference_levels[other_topic] = self.interference_levels.get(other_topic, 0) + interference
                # Appliquer imm√©diatement une petite perte
                self.knowledge[other_topic] = max(0, self.knowledge[other_topic] - interference * 0.5)

    def _get_testing_effect_bonus(self, topic: str) -> float:
        """
        Testing Effect: √™tre test√© am√©liore la r√©tention.
        Plus on a √©t√© test√© sur un topic, meilleure est la r√©tention.
        """
        config = TESTING_EFFECT_CONFIG
        tests_count = self.tested_topics.get(topic, 0)

        # Bonus logarithmique (rendements d√©croissants)
        if tests_count == 0:
            return 0.0
        bonus = config["retention_boost"] * math.log(1 + tests_count) / math.log(10)
        return min(config["retention_boost"], bonus)

    def _record_test(self, topic: str, is_correct: bool):
        """Enregistre qu'on a √©t√© test√© sur ce topic"""
        self.tested_topics[topic] = self.tested_topics.get(topic, 0) + 1

        # Retrieval practice: si correct, on a "r√©cup√©r√©" l'info
        if is_correct:
            self.retrieval_attempts[topic] = self.retrieval_attempts.get(topic, 0) + 1

    def _update_perceived_confidence(self):
        """
        Effet Dunning-Kruger: ajuste la confiance per√ßue selon le niveau r√©el.
        Les d√©butants surestiment, les interm√©diaires sous-estiment.
        """
        config = DUNNING_KRUGER_CONFIG
        avg_mastery = sum(self.knowledge.values()) / max(1, len(self.knowledge)) if self.knowledge else 0

        adjustment = 0.0

        # Zone novice: surestimation
        if config["novice_zone"][0] <= avg_mastery <= config["novice_zone"][1]:
            adjustment = config["novice_overconfidence"]
        # Zone comp√©tent: sous-estimation ("vall√©e du d√©sespoir")
        elif config["competent_zone"][0] <= avg_mastery <= config["competent_zone"][1]:
            adjustment = config["competent_underconfidence"]
        # Zone expert: l√©g√®re sous-estimation
        elif config["expert_zone"][0] <= avg_mastery <= config["expert_zone"][1]:
            adjustment = config["expert_underconfidence"]

        self.perceived_confidence = max(0.1, min(0.95, self.confidence + adjustment))

    def _get_interference_penalty(self, topic: str) -> float:
        """Retourne la p√©nalit√© d'interf√©rence actuelle pour un topic"""
        return self.interference_levels.get(topic, 0.0)

    # =========================================================================
    # v4.4: NOUVELLES M√âTHODES (10/10 ACAD√âMIQUE)
    # =========================================================================

    def _get_desirable_difficulty_modifier(self) -> float:
        """
        Desirable Difficulties (Bjork 1994):
        Une difficult√© "d√©sirable" am√©liore l'apprentissage long terme.
        Zone optimale = 15-30% d'erreurs.
        """
        config = DESIRABLE_DIFFICULTY_CONFIG

        if len(self.recent_errors) < 5:
            return 0.0  # Pas assez de donn√©es

        # Calculer le taux d'erreur r√©cent (derni√®res 10 questions)
        error_rate = sum(1 for e in self.recent_errors[-10:] if e) / len(self.recent_errors[-10:])

        optimal_min, optimal_max = config["optimal_error_rate"]

        if optimal_min <= error_rate <= optimal_max:
            # Zone optimale: bonus d'apprentissage
            return config["optimal_bonus"]
        elif error_rate < optimal_min:
            # Trop facile: p√©nalit√©
            return -config["too_easy_penalty"]
        else:
            # Trop dur: p√©nalit√©
            return -config["too_hard_penalty"]

    def _get_generation_effect_bonus(self) -> float:
        """
        Generation Effect (Slamecka & Graf 1978):
        G√©n√©rer une r√©ponse > recevoir passivement.
        Le mode de r√©ponse affecte l'apprentissage.
        """
        config = GENERATION_EFFECT_CONFIG

        if self.generation_mode == "active":
            return config["active_generation_bonus"]
        elif self.generation_mode == "partial":
            return config["partial_generation_bonus"]
        else:  # passive
            return -config["passive_penalty"]

    def _get_encoding_specificity_modifier(self, topic: str) -> float:
        """
        Encoding Specificity (Tulving & Thomson 1973):
        On se souvient mieux si le contexte de test ‚âà contexte d'apprentissage.
        """
        config = ENCODING_SPECIFICITY_CONFIG

        if topic not in self.learning_contexts:
            return 0.0  # Premi√®re fois sur ce topic

        learned_ctx = self.learning_contexts[topic]
        current_ctx = {
            "hour": self.last_session_hour,
            "motivation": self.current_motivation,
            "fatigue": self.fatigue
        }

        # Calculer la similarit√© de contexte
        # Heure: diff√©rence normalis√©e (0-12h de diff max)
        hour_diff = abs(learned_ctx.get("hour", 12) - current_ctx["hour"])
        hour_similarity = 1 - min(hour_diff, 12) / 12

        # √âtat: diff√©rence de motivation et fatigue
        motivation_diff = abs(learned_ctx.get("motivation", 0.5) - current_ctx["motivation"])
        fatigue_diff = abs(learned_ctx.get("fatigue", 0.5) - current_ctx["fatigue"])
        state_similarity = 1 - (motivation_diff + fatigue_diff) / 2

        # Score de similarit√© pond√©r√©
        similarity = (
            config["time_consistency_weight"] * hour_similarity +
            config["state_consistency_weight"] * state_similarity
        )

        # Retourner bonus ou malus selon la similarit√©
        if similarity > 0.7:
            return config["context_match_bonus"]
        elif similarity < 0.4:
            return -config["context_mismatch_penalty"]
        return 0.0

    def _get_attention_fluctuation_modifier(self) -> float:
        """
        Attention Fluctuante (Cycles Ultradian):
        L'attention varie toutes les ~20 minutes.
        """
        config = ATTENTION_FLUCTUATION_CONFIG

        # Calculer o√π on en est dans le cycle (bas√© sur le nombre de questions)
        # ~2 questions par minute = cycle de 40 questions
        questions_in_cycle = (self.session_start_minute * 2 + self.session_questions) % 40

        # Creux d'attention autour de questions 15-25 (milieu du cycle)
        if 15 <= questions_in_cycle <= 25:
            if not self.in_attention_trough:
                self.in_attention_trough = True
                self.questions_since_trough = 0
            return -config["trough_penalty"]
        else:
            # R√©cup√©ration apr√®s un creux
            if self.in_attention_trough:
                self.questions_since_trough += 1
                if self.questions_since_trough >= config["recovery_questions"]:
                    self.in_attention_trough = False

            # Pic d'attention en d√©but de cycle
            if questions_in_cycle < 10:
                return config["peak_bonus"]

        return 0.0

    def _update_recent_errors(self, is_correct: bool):
        """Met √† jour la liste des erreurs r√©centes (pour Desirable Difficulties)"""
        self.recent_errors.append(not is_correct)
        if len(self.recent_errors) > 10:
            self.recent_errors.pop(0)

    def _update_learning_context(self, topic: str):
        """Sauvegarde le contexte d'apprentissage (pour Encoding Specificity)"""
        self.learning_contexts[topic] = {
            "hour": self.last_session_hour,
            "motivation": self.current_motivation,
            "fatigue": self.fatigue
        }

    def _simulate_generation_mode(self):
        """Simule le type de question (active, passive, partial)"""
        # 60% active (r√©ponse libre), 30% partial (compl√©tion), 10% passive (QCM)
        roll = random.random()
        if roll < 0.60:
            self.generation_mode = "active"
        elif roll < 0.90:
            self.generation_mode = "partial"
        else:
            self.generation_mode = "passive"

    def should_skip_session(self) -> bool:
        """D√©cide si l'√©l√®ve saute la session"""
        return random.random() < self.profile.session_skip_prob

    def should_quit_early(self) -> tuple:
        """D√©cide si l'√©l√®ve abandonne en cours de session"""
        prob = self.profile.early_quit_prob
        prob += self.fatigue * 0.1
        prob += (1 - self.current_motivation) * 0.1

        if random.random() < prob:
            reasons = [
                "Distraction",
                "Fatigue",
                "Perte de motivation",
                "Autre priorit√©"
            ]
            return True, random.choice(reasons)
        return False, None

    def answer_question(self, topic: str, difficulty: int) -> dict:
        """
        Simule une r√©ponse avec:
        - Variabilit√© des questions (pi√®ges/faciles)
        - Effet warmup (premi√®re question)
        - v4.2: Confiance affecte la prise de risque
        - v4.2: Plateau r√©duit la performance
        - v4.3: Testing Effect (bonus r√©tention)
        - v4.3: Interf√©rence proactive
        - v4.3: Dunning-Kruger (confiance per√ßue)
        - v4.4: Encoding Specificity (contexte)
        - v4.4: Attention Fluctuante (cycles ultradian)
        """
        mastery = self.knowledge.get(topic, 0.0)
        p = self.profile

        # v4.4: Simuler le type de question (active, passive, partial)
        self._simulate_generation_mode()

        # Probabilit√© de base
        base_prob = p.base_ability + mastery * 0.3

        # Malus difficult√©
        diff_penalty = (difficulty - 1) * 0.08

        # Bonus/malus motivation et focus
        motivation_bonus = (self.current_motivation - 0.5) * 0.15
        focus_bonus = (p.focus - 0.5) * 0.1

        # Malus fatigue
        fatigue_penalty = self.fatigue * 0.15

        # v4.2: Bonus/malus confiance
        confidence_bonus = (self.confidence - 0.5) * 0.10

        # v4.3: Testing Effect - bonus si d√©j√† test√© sur ce topic
        testing_bonus = self._get_testing_effect_bonus(topic)

        # v4.3: Interf√©rence proactive - malus si topics similaires ma√Ætris√©s
        interference_penalty = self._apply_proactive_interference(topic)

        # v4.3: Interf√©rence actuelle (r√©troactive accumul√©e)
        current_interference = self._get_interference_penalty(topic)

        # v4.4: Encoding Specificity - bonus/malus selon similarit√© contexte
        encoding_modifier = self._get_encoding_specificity_modifier(topic)

        # v4.4: Attention Fluctuante - bonus/malus selon cycle ultradian
        attention_modifier = self._get_attention_fluctuation_modifier()

        prob_correct = (base_prob - diff_penalty + motivation_bonus + focus_bonus
                       - fatigue_penalty + confidence_bonus + testing_bonus
                       + interference_penalty - current_interference
                       + encoding_modifier + attention_modifier)

        # v4.1: Effet warmup (premi√®re question du jour)
        if self.is_first_question_of_day:
            prob_correct -= WARMUP_PENALTY
            self.is_first_question_of_day = False

        # v4.2: Malus plateau (stagnation = moins de r√©ussite)
        if self.in_plateau:
            prob_correct -= 0.10  # -10% pendant un plateau

        # v4.1: Variabilit√© des questions
        variance = QUESTION_VARIANCE.get(difficulty, QUESTION_VARIANCE[3])
        roll = random.random()
        if roll < variance["trap_prob"]:
            # Question pi√®ge: -20% de chance
            prob_correct -= 0.20
        elif roll < variance["trap_prob"] + variance["easy_prob"]:
            # Question intuitive: +10% de chance
            prob_correct += 0.10

        prob_correct = max(0.10, min(0.95, prob_correct))

        is_correct = random.random() < prob_correct

        # v4.3: Enregistrer le test (Testing Effect)
        self._record_test(topic, is_correct)

        # v4.2: Mettre √† jour la confiance
        self._update_confidence(is_correct)

        # v4.4: Mettre √† jour les erreurs r√©centes (pour Desirable Difficulties)
        self._update_recent_errors(is_correct)

        # Temps de r√©ponse
        base_time = 8 + difficulty * 4
        base_time *= (1.2 - p.focus * 0.3)
        base_time *= (1 + self.fatigue * 0.4)
        # v4.2: Confiance basse = plus lent (h√©sitation)
        if self.confidence < 0.3:
            base_time *= 1.2
        # v4.3: Dunning-Kruger - confiance per√ßue affecte le temps de r√©ponse
        if self.perceived_confidence > self.confidence + 0.1:
            # Surconfiant = r√©pond trop vite (parfois erreurs)
            base_time *= 0.85
        # v4.4: Attention fluctuante affecte aussi le temps
        if self.in_attention_trough:
            base_time *= 1.15  # +15% temps de r√©ponse dans un creux
        response_time = base_time + random.uniform(-3, 8)
        response_time = max(3, response_time)

        # Augmenter fatigue
        self.fatigue = min(1.0, self.fatigue + 0.03 * (1 - p.fatigue_resistance))
        self.session_questions += 1

        return {
            "is_correct": is_correct,
            "response_time": response_time,
            "prob": prob_correct,
            "confidence": self.confidence,
            "perceived_confidence": self.perceived_confidence,
            "in_plateau": self.in_plateau,
            "testing_bonus": testing_bonus,
            "interference": current_interference,
            # v4.4
            "encoding_modifier": encoding_modifier,
            "attention_modifier": attention_modifier,
            "generation_mode": self.generation_mode,
            "in_attention_trough": self.in_attention_trough
        }

    def learn(self, topic: str, is_correct: bool, difficulty: int):
        """
        Apprentissage avec:
        - v4.1: Transfert inter-topics
        - v4.2: Spacing effect (r√©vision espac√©e = meilleur apprentissage)
        - v4.2: Plateau (r√©duction de l'apprentissage pendant stagnation)
        - v4.3: Testing Effect (bonus r√©tention)
        - v4.3: Interf√©rence r√©troactive
        - v4.3: Consolidation nocturne (bonus matin)
        - v4.4: Desirable Difficulties (zone optimale d'erreurs)
        - v4.4: Generation Effect (g√©n√©ration active > passive)
        - v4.4: Encoding Specificity (contexte d'apprentissage)
        - v4.4: Attention Fluctuante (cycles ultradian)
        """
        current = self.knowledge.get(topic, 0.0)
        p = self.profile

        efficiency = p.learning_speed * p.focus * (0.8 + self.current_motivation * 0.4)
        efficiency *= (1 - self.fatigue * 0.3)

        # v4.2: Spacing effect - bonus si r√©vision espac√©e optimalement
        spacing_multiplier = self._get_spacing_bonus(topic)
        efficiency *= spacing_multiplier

        # v4.3: Testing Effect - le fait d'√™tre test√© am√©liore l'apprentissage
        testing_multiplier = 1.0 + self._get_testing_effect_bonus(topic)
        efficiency *= testing_multiplier

        # v4.3: Bonus consolidation nocturne si session le matin
        config = SLEEP_CONSOLIDATION_CONFIG
        if config["morning_hours"][0] <= self.last_session_hour <= config["morning_hours"][1]:
            efficiency *= (1 + config["morning_bonus"])

        # v4.2: Plateau - r√©duction de l'apprentissage pendant stagnation
        if self.in_plateau:
            efficiency *= 0.50  # -50% d'apprentissage pendant plateau

        # v4.4: Desirable Difficulties - bonus/malus selon le taux d'erreur r√©cent
        desirable_diff_modifier = self._get_desirable_difficulty_modifier()
        efficiency *= (1 + desirable_diff_modifier)

        # v4.4: Generation Effect - bonus si g√©n√©ration active
        generation_bonus = self._get_generation_effect_bonus()
        efficiency *= (1 + generation_bonus)

        # v4.4: Attention Fluctuante - malus si dans un creux d'attention
        if self.in_attention_trough:
            efficiency *= 0.85  # -15% d'apprentissage dans un creux

        gain = 0.0
        if is_correct:
            gain = 0.06 * efficiency * (1 + difficulty * 0.1)
            self.knowledge[topic] = min(1.0, current + gain)
        else:
            # L√©g√®re progression m√™me sur erreur (pour √©l√®ves motiv√©s)
            # v4.3: Testing Effect - m√™me les erreurs aident √† apprendre
            if self.current_motivation > 0.6:
                gain = 0.015 * efficiency
                # Bonus testing effect sur erreur (retrieval practice)
                gain *= (1 + TESTING_EFFECT_CONFIG["retrieval_practice_bonus"] * 0.5)
                self.knowledge[topic] = min(1.0, current + gain)

        # Marquer le topic comme pratiqu√© aujourd'hui
        self.last_practice_day[topic] = self.current_day

        # v4.4: Sauvegarder le contexte d'apprentissage (pour Encoding Specificity)
        self._update_learning_context(topic)

        # v4.3: Interf√©rence r√©troactive - apprendre ce topic peut perturber les autres
        if gain > 0:
            self._apply_retroactive_interference(topic, gain)

        # v4.1: Transfert inter-topics
        if gain > 0 and topic in TOPIC_TRANSFER_MATRIX:
            transfers = TOPIC_TRANSFER_MATRIX[topic]
            for other_topic, transfer_rate in transfers.items():
                if other_topic not in self.knowledge:
                    self.knowledge[other_topic] = 0.0
                transfer_gain = gain * transfer_rate
                self.knowledge[other_topic] = min(1.0, self.knowledge[other_topic] + transfer_gain)


# =============================================================================
# SIMULATION
# =============================================================================

def run_simulation(profile_key: str, num_days: int = 14, questions_per_day: int = 15):
    """Ex√©cute une simulation pour un profil donn√©"""

    profile = PROFILES[profile_key]
    student = SimulatedStudent(profile)
    user_id = f"sim_{profile_key}"
    topics = ["conjugaison", "grammaire", "vocabulaire", "orthographe"]

    print(f"\n{'='*70}")
    print(f"üë§ {profile.name}")
    print(f"   {profile.description}")
    print(f"{'='*70}")

    # Stats
    daily_stats = []
    total_xp = 0
    sessions_skipped = 0
    early_quits = 0

    for day in range(1, num_days + 1):
        # V√©rifier si l'√©l√®ve fait la session
        if student.should_skip_session():
            sessions_skipped += 1
            # NOUVEAU: Appliquer l'oubli m√™me si session saut√©e
            student.apply_day_skip(day)
            avg_mastery = sum(student.knowledge.values()) / max(1, len(student.knowledge)) if student.knowledge else 0
            print(f"   Jour {day}: ‚è≠Ô∏è Session saut√©e (ma√Ætrise apr√®s oubli: {avg_mastery*100:.0f}%)")
            daily_stats.append({
                "day": day,
                "skipped": True,
                "accuracy": None,
                "mastery": avg_mastery
            })
            continue

        # D√©but de session (avec oubli appliqu√©)
        # v4.3: Reset sleep_consolidated pour le nouveau jour
        student.sleep_consolidated = False
        student.start_session(day)
        lean_engine.reset_session(user_id)

        day_correct = 0
        day_total = 0
        day_xp = 0

        for q_num in range(questions_per_day):
            # V√©rifier abandon pr√©coce
            quit_early, reason = student.should_quit_early()
            if quit_early:
                early_quits += 1
                if day <= 3:
                    print(f"   Jour {day}: üö™ Abandon apr√®s {q_num} questions ({reason})")
                break

            # Choisir topic (interleaving)
            topic = topics[q_num % len(topics)]
            mastery = int(student.knowledge.get(topic, 0.0) * 100)

            # Param√®tres optimaux
            params = lean_engine.get_next_question(user_id, topic, mastery)

            # R√©ponse
            response = student.answer_question(topic, params.difficulty)

            # Traiter
            result = lean_engine.process_answer(
                user_id=user_id,
                topic_id=topic,
                is_correct=response["is_correct"],
                response_time=response["response_time"],
                difficulty=params.difficulty
            )

            # Apprentissage
            student.learn(topic, response["is_correct"], params.difficulty)

            day_total += 1
            if response["is_correct"]:
                day_correct += 1
            day_xp += result.xp_earned
            total_xp += result.xp_earned

        # Stats du jour
        if day_total > 0:
            accuracy = day_correct / day_total
            avg_mastery = sum(student.knowledge.values()) / len(student.knowledge) if student.knowledge else 0

            daily_stats.append({
                "day": day,
                "skipped": False,
                "accuracy": accuracy,
                "mastery": avg_mastery,
                "questions": day_total,
                "xp": day_xp
            })

            if day <= 3 or day == num_days:
                print(f"   Jour {day}: ‚úÖ {accuracy*100:.0f}% pr√©cision, {avg_mastery*100:.0f}% ma√Ætrise, {day_xp} XP")

    # R√©sultats
    completed_days = [d for d in daily_stats if not d.get("skipped", False)]

    if completed_days:
        final_mastery = completed_days[-1]["mastery"]
        final_accuracy = sum(d["accuracy"] for d in completed_days) / len(completed_days)
        total_questions = sum(d.get("questions", 0) for d in completed_days)
    else:
        final_mastery = 0
        final_accuracy = 0
        total_questions = 0

    return {
        "profile": profile.name,
        "final_mastery": final_mastery,
        "final_accuracy": final_accuracy,
        "total_xp": total_xp,
        "total_questions": total_questions,
        "sessions_completed": len(completed_days),
        "sessions_skipped": sessions_skipped,
        "early_quits": early_quits,
        "days": num_days
    }


def run_all_simulations():
    """Ex√©cute les simulations pour tous les profils"""

    print("\n" + "=" * 70)
    print("üß™ SIMULATION DES 4 PROFILS √âL√àVES - MOTEUR LEAN v4.4 (10/10 ACAD√âMIQUE)")
    print("=" * 70)

    # Info moteur
    info = lean_engine.get_engine_info()
    print(f"\nüìä Moteur: {info['version']} ({info['modules']} modules)")

    results = []

    for profile_key in ["determined", "average", "irregular", "struggling"]:
        result = run_simulation(profile_key, num_days=14, questions_per_day=15)
        results.append(result)

    # Tableau comparatif
    print("\n" + "=" * 70)
    print("üìä COMPARATIF DES PROFILS")
    print("=" * 70)

    print(f"\n{'Profil':<25} {'Ma√Ætrise':>10} {'Pr√©cision':>10} {'Sessions':>10} {'XP':>8}")
    print("-" * 70)

    for r in results:
        sessions_str = f"{r['sessions_completed']}/{r['days']}"
        print(f"{r['profile']:<25} {r['final_mastery']*100:>9.0f}% {r['final_accuracy']*100:>9.0f}% {sessions_str:>10} {r['total_xp']:>8}")

    # Analyse
    print("\n" + "=" * 70)
    print("üîç ANALYSE")
    print("=" * 70)

    determined = results[0]
    average = results[1]
    irregular = results[2]
    struggling = results[3]

    print(f"""
‚úÖ D√âTERMIN√â ({determined['profile']}):
   ‚Üí Ma√Ætrise: {determined['final_mastery']*100:.0f}% (objectif: >90%)
   ‚Üí Le syst√®me permet d'atteindre l'excellence

üìä MOYEN ({average['profile']}):
   ‚Üí Ma√Ætrise: {average['final_mastery']*100:.0f}% (objectif: 60-80%)
   ‚Üí Progression r√©aliste pour l'utilisateur type

üîÑ IRR√âGULIER ({irregular['profile']}):
   ‚Üí Ma√Ætrise: {irregular['final_mastery']*100:.0f}% (sessions: {irregular['sessions_completed']}/{irregular['days']})
   ‚Üí Progression malgr√© les sessions manqu√©es

‚ö†Ô∏è EN DIFFICULT√â ({struggling['profile']}):
   ‚Üí Ma√Ætrise: {struggling['final_mastery']*100:.0f}% (objectif: progression visible)
   ‚Üí Le syst√®me s'adapte au rythme plus lent
""")

    # Verdict
    all_progressed = all(r['final_mastery'] > 0.2 for r in results)
    determined_excellent = determined['final_mastery'] > 0.85
    average_good = 0.5 < average['final_mastery'] < 0.9
    struggling_progressed = struggling['final_mastery'] > 0.25

    print("=" * 70)
    if all_progressed and determined_excellent and struggling_progressed:
        print("‚úÖ TOUS LES PROFILS PROGRESSENT - Syst√®me valid√©!")
    else:
        print("‚ö†Ô∏è AJUSTEMENTS N√âCESSAIRES:")
        if not determined_excellent:
            print("   - D√©termin√© n'atteint pas 85%+")
        if not average_good:
            print("   - Moyen hors de la zone 50-90%")
        if not struggling_progressed:
            print("   - En difficult√© ne progresse pas assez")
    print("=" * 70)

    return results


if __name__ == "__main__":
    results = run_all_simulations()
    sys.exit(0)
